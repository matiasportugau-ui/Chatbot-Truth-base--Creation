"use strict";
/**
 * Moderation guardrail for text content using OpenAI's moderation API.
 *
 * This module provides a guardrail for detecting harmful or policy-violating content
 * using OpenAI's moderation API. It supports filtering by specific content categories
 * and provides detailed analysis of detected violations.
 *
 * Configuration Parameters:
 * `categories` (Category[]): List of moderation categories to check.
 *
 * Available categories listed below. If not specified, all categories are checked by default.
 *
 * Example:
 * ```typescript
 * const cfg = { categories: ["hate", "harassment", "self-harm"] };
 * const result = await moderationCheck(null, "harmful content here", cfg);
 * console.log(result.tripwireTriggered); // true
 * ```
 */
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.moderationCheck = exports.ModerationContext = exports.ModerationConfigRequired = exports.ModerationConfig = exports.Category = void 0;
const zod_1 = require("zod");
const registry_1 = require("../registry");
const openai_1 = __importDefault(require("openai"));
const safety_identifier_1 = require("../utils/safety-identifier");
/**
 * Enumeration of supported moderation categories.
 *
 * These categories correspond to types of harmful or restricted content
 * recognized by the OpenAI moderation endpoint.
 */
var Category;
(function (Category) {
    Category["SEXUAL"] = "sexual";
    Category["SEXUAL_MINORS"] = "sexual/minors";
    Category["HATE"] = "hate";
    Category["HATE_THREATENING"] = "hate/threatening";
    Category["HARASSMENT"] = "harassment";
    Category["HARASSMENT_THREATENING"] = "harassment/threatening";
    Category["SELF_HARM"] = "self-harm";
    Category["SELF_HARM_INTENT"] = "self-harm/intent";
    Category["SELF_HARM_INSTRUCTIONS"] = "self-harm/instructions";
    Category["VIOLENCE"] = "violence";
    Category["VIOLENCE_GRAPHIC"] = "violence/graphic";
    Category["ILLICIT"] = "illicit";
    Category["ILLICIT_VIOLENT"] = "illicit/violent";
})(Category || (exports.Category = Category = {}));
/**
 * Configuration schema for the moderation guardrail.
 *
 * This configuration allows selection of specific moderation categories to check.
 * If no categories are specified, all supported categories will be checked.
 */
exports.ModerationConfig = zod_1.z.object({
    /** List of moderation categories to check. Defaults to all categories if not specified. */
    categories: zod_1.z.array(zod_1.z.nativeEnum(Category)).default(Object.values(Category)),
});
// Schema for registry registration (with defaults)
exports.ModerationConfigRequired = zod_1.z
    .object({
    categories: zod_1.z.array(zod_1.z.nativeEnum(Category)),
})
    .transform((data) => ({
    ...data,
    categories: data.categories ?? Object.values(Category),
}));
/**
 * Context requirements for the moderation guardrail.
 */
exports.ModerationContext = zod_1.z.object({
    /** Optional OpenAI client to reuse instead of creating a new one */
    guardrailLlm: zod_1.z.unknown().optional(),
});
/**
 * Check if an error is a 404 Not Found error from the OpenAI API.
 *
 * @param error The error to check
 * @returns True if the error is a 404 error
 */
function isNotFoundError(error) {
    return !!(error && typeof error === 'object' && 'status' in error && error.status === 404);
}
/**
 * Ensure a value is an Error instance.
 * Converts non-Error values to Error instances with a string representation.
 *
 * @param error The error value to convert
 * @returns An Error instance
 */
function ensureError(error) {
    return error instanceof Error ? error : new Error(String(error));
}
/**
 * Call the OpenAI moderation API.
 *
 * @param client The OpenAI client to use
 * @param data The text to analyze
 * @returns The moderation API response
 */
function callModerationAPI(client, data) {
    const params = {
        model: 'omni-moderation-latest',
        input: data,
    };
    // Only include safety_identifier for official OpenAI API (not Azure or local providers)
    if ((0, safety_identifier_1.supportsSafetyIdentifier)(client)) {
        // @ts-ignore - safety_identifier is not defined in OpenAI types yet
        params.safety_identifier = safety_identifier_1.SAFETY_IDENTIFIER;
    }
    // @ts-ignore - safety_identifier is not in the OpenAI types yet
    return client.moderations.create(params);
}
/**
 * Guardrail check_fn to flag disallowed content categories using OpenAI moderation API.
 *
 * Calls the OpenAI moderation endpoint on input text and flags if any of the
 * configured categories are detected. Returns a result containing flagged
 * categories, details, and tripwire status.
 *
 * @param ctx Runtime context (unused)
 * @param data User or model text to analyze
 * @param config Moderation config specifying categories to flag
 * @returns GuardrailResult indicating if tripwire was triggered, and details of flagged categories
 */
const moderationCheck = async (ctx, data, config) => {
    // Handle the case where config might be wrapped in another object
    const actualConfig = config.config || config;
    // Ensure categories is an array
    const configObj = actualConfig;
    const categories = configObj.categories || Object.values(Category);
    // Get client from context if available
    let client = null;
    if (ctx) {
        const contextObj = ctx;
        const candidate = contextObj.guardrailLlm;
        // Just use whatever is provided, let the try-catch handle validation
        if (candidate) {
            client = candidate;
        }
    }
    try {
        // Try the context client first, fall back if moderation endpoint doesn't exist
        let resp;
        if (client !== null) {
            try {
                resp = await callModerationAPI(client, data);
            }
            catch (error) {
                // Moderation endpoint doesn't exist on this provider (e.g., third-party)
                // Fall back to the OpenAI client
                if (isNotFoundError(error)) {
                    try {
                        resp = await callModerationAPI(new openai_1.default(), data);
                    }
                    catch (fallbackError) {
                        // If fallback fails, return execution failure
                        // This allows runGuardrails to handle based on raiseGuardrailErrors flag
                        const errorObj = ensureError(fallbackError);
                        return {
                            tripwireTriggered: false,
                            executionFailed: true,
                            originalException: errorObj,
                            info: {
                                checked_text: data,
                                error: errorObj.message,
                            },
                        };
                    }
                }
                else {
                    // Non-404 error from context client - return execution failure
                    // This allows runGuardrails to handle based on raiseGuardrailErrors flag
                    const errorObj = ensureError(error);
                    return {
                        tripwireTriggered: false,
                        executionFailed: true,
                        originalException: errorObj,
                        info: {
                            checked_text: data,
                            error: errorObj.message,
                        },
                    };
                }
            }
        }
        else {
            // No context client, use fallback
            resp = await callModerationAPI(new openai_1.default(), data);
        }
        const results = resp.results || [];
        if (!results.length) {
            return {
                tripwireTriggered: false,
                info: {
                    error: 'No moderation results returned',
                },
            };
        }
        const outcome = results[0];
        const moderationCategories = outcome.categories || {};
        // Check only the categories specified in config and collect results
        const flaggedCategories = [];
        const categoryDetails = {};
        for (const cat of categories) {
            const catValue = cat;
            const isFlagged = moderationCategories[catValue] || false;
            if (isFlagged) {
                flaggedCategories.push(catValue);
            }
            categoryDetails[catValue] = isFlagged;
        }
        // Only trigger if the requested categories are flagged
        const isFlagged = flaggedCategories.length > 0;
        return {
            tripwireTriggered: isFlagged,
            info: {
                guardrail_name: 'Moderation',
                flagged_categories: flaggedCategories,
                categories_checked: categories,
                category_details: categoryDetails,
            },
        };
    }
    catch (error) {
        console.warn('AI-based moderation failed:', error);
        const errorObj = ensureError(error);
        return {
            tripwireTriggered: false,
            executionFailed: true,
            originalException: errorObj,
            info: {
                checked_text: data,
                error: errorObj.message,
            },
        };
    }
};
exports.moderationCheck = moderationCheck;
// Auto-register this guardrail with the default registry
registry_1.defaultSpecRegistry.register('Moderation', exports.moderationCheck, 'Flags text containing disallowed content categories', 'text/plain', exports.ModerationConfigRequired, exports.ModerationContext, { engine: 'API' });
//# sourceMappingURL=moderation.js.map