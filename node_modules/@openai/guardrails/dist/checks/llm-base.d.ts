/**
 * LLM-based guardrail content checking.
 *
 * This module enables the creation and registration of content moderation guardrails
 * using Large Language Models (LLMs). It provides configuration and output schemas,
 * prompt helpers, a utility for executing LLM-based checks, and a factory for generating
 * guardrail check functions leveraging LLMs.
 */
import { z, ZodTypeAny } from 'zod';
import { OpenAI } from 'openai';
import { CheckFn, GuardrailResult, GuardrailLLMContext } from '../types';
/**
 * Configuration schema for LLM-based content checks.
 *
 * Used to specify the LLM model and confidence threshold for triggering a tripwire.
 */
export declare const LLMConfig: z.ZodObject<{
    /** The LLM model to use for checking the text */
    model: z.ZodString;
    /** Minimum confidence required to trigger the guardrail, as a float between 0.0 and 1.0 */
    confidence_threshold: z.ZodDefault<z.ZodNumber>;
    /** Optional system prompt details for user-defined LLM guardrails */
    system_prompt_details: z.ZodOptional<z.ZodString>;
}, "strip", z.ZodTypeAny, {
    model: string;
    confidence_threshold: number;
    system_prompt_details?: string | undefined;
}, {
    model: string;
    confidence_threshold?: number | undefined;
    system_prompt_details?: string | undefined;
}>;
export type LLMConfig = z.infer<typeof LLMConfig>;
/**
 * Output schema for LLM content checks.
 *
 * Used for structured results returned by LLM-based moderation guardrails.
 */
export declare const LLMOutput: z.ZodObject<{
    /** Indicates whether the content was flagged */
    flagged: z.ZodBoolean;
    /** LLM's confidence in the flagging decision (0.0 to 1.0) */
    confidence: z.ZodNumber;
}, "strip", z.ZodTypeAny, {
    flagged: boolean;
    confidence: number;
}, {
    flagged: boolean;
    confidence: number;
}>;
export type LLMOutput = z.infer<typeof LLMOutput>;
/**
 * Extended LLM output schema with error information.
 *
 * Extends LLMOutput to include additional information about errors that occurred
 * during LLM processing, such as content filter triggers.
 */
export declare const LLMErrorOutput: z.ZodObject<{
    /** Indicates whether the content was flagged */
    flagged: z.ZodBoolean;
    /** LLM's confidence in the flagging decision (0.0 to 1.0) */
    confidence: z.ZodNumber;
} & {
    /** Additional information about the error */
    info: z.ZodRecord<z.ZodString, z.ZodAny>;
}, "strip", z.ZodTypeAny, {
    info: Record<string, any>;
    flagged: boolean;
    confidence: number;
}, {
    info: Record<string, any>;
    flagged: boolean;
    confidence: number;
}>;
export type LLMErrorOutput = z.infer<typeof LLMErrorOutput>;
/**
 * Create a standardized error result for LLM-based guardrails.
 *
 * This helper provides a consistent way to handle errors across all LLM-based checks,
 * ensuring uniform error reporting and preventing tripwire triggers on execution failures.
 * Sets executionFailed=true to enable raiseGuardrailErrors handling.
 *
 * @param guardrailName - Name of the guardrail that encountered the error.
 * @param analysis - LLMErrorOutput containing error information.
 * @param additionalInfo - Optional additional information to include in the result.
 * @returns GuardrailResult with tripwireTriggered=false, executionFailed=true, and error information.
 */
export declare function createErrorResult(guardrailName: string, analysis: LLMErrorOutput, additionalInfo?: Record<string, unknown>): GuardrailResult;
export declare function buildFullPrompt(systemPrompt: string, outputModel?: ZodTypeAny): string;
/**
 * Run an LLM analysis for a given prompt and user input.
 *
 * Invokes the OpenAI LLM, enforces prompt/response contract, parses the LLM's
 * output, and returns a validated result.
 *
 * @param text - Text to analyze.
 * @param systemPrompt - Prompt instructions for the LLM.
 * @param client - OpenAI client for LLM inference.
 * @param model - Identifier for which LLM model to use.
 * @param outputModel - Model for parsing and validating the LLM's response.
 * @returns Structured output containing the detection decision and confidence.
 */
export declare function runLLM<TOutput extends ZodTypeAny>(text: string, systemPrompt: string, client: OpenAI, model: string, outputModel: TOutput): Promise<z.infer<TOutput> | LLMErrorOutput>;
export declare function createLLMCheckFn(name: string, description: string, systemPrompt: string, outputModel?: typeof LLMOutput, configModel?: typeof LLMConfig): CheckFn<GuardrailLLMContext, string, z.infer<typeof LLMConfig>>;
/**
 * Context requirements for LLM-based guardrails.
 */
export declare const LLMContext: z.ZodType<GuardrailLLMContext>;
//# sourceMappingURL=llm-base.d.ts.map