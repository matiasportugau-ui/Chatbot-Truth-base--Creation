"use strict";
/**
 * LLM-based guardrail content checking.
 *
 * This module enables the creation and registration of content moderation guardrails
 * using Large Language Models (LLMs). It provides configuration and output schemas,
 * prompt helpers, a utility for executing LLM-based checks, and a factory for generating
 * guardrail check functions leveraging LLMs.
 */
Object.defineProperty(exports, "__esModule", { value: true });
exports.LLMContext = exports.LLMErrorOutput = exports.LLMOutput = exports.LLMConfig = void 0;
exports.createErrorResult = createErrorResult;
exports.buildFullPrompt = buildFullPrompt;
exports.runLLM = runLLM;
exports.createLLMCheckFn = createLLMCheckFn;
const zod_1 = require("zod");
const registry_1 = require("../registry");
const safety_identifier_1 = require("../utils/safety-identifier");
/**
 * Configuration schema for LLM-based content checks.
 *
 * Used to specify the LLM model and confidence threshold for triggering a tripwire.
 */
exports.LLMConfig = zod_1.z.object({
    /** The LLM model to use for checking the text */
    model: zod_1.z.string().describe('LLM model to use for checking the text'),
    /** Minimum confidence required to trigger the guardrail, as a float between 0.0 and 1.0 */
    confidence_threshold: zod_1.z
        .number()
        .min(0.0)
        .max(1.0)
        .default(0.7)
        .describe('Minimum confidence threshold to trigger the guardrail (0.0 to 1.0). Defaults to 0.7.'),
    /** Optional system prompt details for user-defined LLM guardrails */
    system_prompt_details: zod_1.z.string().optional().describe('Additional system prompt details'),
});
/**
 * Output schema for LLM content checks.
 *
 * Used for structured results returned by LLM-based moderation guardrails.
 */
exports.LLMOutput = zod_1.z.object({
    /** Indicates whether the content was flagged */
    flagged: zod_1.z.boolean(),
    /** LLM's confidence in the flagging decision (0.0 to 1.0) */
    confidence: zod_1.z.number().min(0.0).max(1.0),
});
/**
 * Extended LLM output schema with error information.
 *
 * Extends LLMOutput to include additional information about errors that occurred
 * during LLM processing, such as content filter triggers.
 */
exports.LLMErrorOutput = exports.LLMOutput.extend({
    /** Additional information about the error */
    info: zod_1.z.record(zod_1.z.string(), zod_1.z.any()),
});
/**
 * Create a standardized error result for LLM-based guardrails.
 *
 * This helper provides a consistent way to handle errors across all LLM-based checks,
 * ensuring uniform error reporting and preventing tripwire triggers on execution failures.
 * Sets executionFailed=true to enable raiseGuardrailErrors handling.
 *
 * @param guardrailName - Name of the guardrail that encountered the error.
 * @param analysis - LLMErrorOutput containing error information.
 * @param additionalInfo - Optional additional information to include in the result.
 * @returns GuardrailResult with tripwireTriggered=false, executionFailed=true, and error information.
 */
function createErrorResult(guardrailName, analysis, additionalInfo = {}) {
    return {
        tripwireTriggered: false,
        executionFailed: true,
        originalException: new Error(String(analysis.info?.error_message || 'LLM execution failed')),
        info: {
            guardrail_name: guardrailName,
            flagged: analysis.flagged,
            confidence: analysis.confidence,
            ...analysis.info,
            ...additionalInfo,
        },
    };
}
/**
 * Assemble a complete LLM prompt with instructions and response schema.
 *
 * Incorporates the supplied system prompt and specifies the required JSON response fields.
 *
 * @param systemPrompt - The instructions describing analysis criteria.
 * @returns Formatted prompt string for LLM input.
 */
function unwrapSchema(schema) {
    if ('unwrap' in schema && typeof schema.unwrap === 'function') {
        return unwrapSchema(schema.unwrap());
    }
    const def = schema._def;
    if (!def) {
        return schema;
    }
    if (def.innerType) {
        return unwrapSchema(def.innerType);
    }
    if (def.schema) {
        return unwrapSchema(def.schema);
    }
    if (def.type) {
        return unwrapSchema(def.type);
    }
    return schema;
}
function describeSchemaType(schema) {
    const base = unwrapSchema(schema);
    if (base instanceof zod_1.z.ZodBoolean) {
        return 'boolean';
    }
    if (base instanceof zod_1.z.ZodNumber) {
        return 'float';
    }
    if (base instanceof zod_1.z.ZodString) {
        return 'string';
    }
    if (base instanceof zod_1.z.ZodArray) {
        return 'array';
    }
    if (base instanceof zod_1.z.ZodObject) {
        return 'object';
    }
    return 'value';
}
function formatFieldInstruction(fieldName, schema) {
    if (fieldName === 'flagged') {
        return '- "flagged": boolean (true if detected and false otherwise)';
    }
    if (fieldName === 'confidence') {
        return '- "confidence": float (0.0 to 1.0)';
    }
    if (fieldName === 'reason') {
        return '- "reason": string explaining the rationale for the decision';
    }
    const valueType = describeSchemaType(schema);
    return `- "${fieldName}": ${valueType}`;
}
function buildFieldInstructionBlock(outputModel) {
    if (!outputModel || !(outputModel instanceof zod_1.z.ZodObject)) {
        return null;
    }
    const shape = outputModel.shape;
    const instructions = Object.entries(shape).map(([key, schema]) => formatFieldInstruction(key, schema));
    return instructions.join('\n');
}
function buildFullPrompt(systemPrompt, outputModel) {
    // Check if the system prompt already contains JSON output format instructions
    // Look for phrases that indicate output formatting requirements, not just mentions of JSON
    const hasJsonOutputInstructions = /(?:respond|output|return)\s+(?:with\s+)?(?:a\s+)?json|format.*json/i.test(systemPrompt);
    if (hasJsonOutputInstructions) {
        // If the system prompt already has detailed JSON instructions, use it as-is
        return systemPrompt;
    }
    const fieldInstructions = buildFieldInstructionBlock(outputModel);
    // Default template for simple cases - always include "json" for OpenAI's response_format requirement
    const template = `
${systemPrompt}

Respond with a json object containing:
${fieldInstructions ?? '- "flagged": boolean (true if detected and false otherwise)\n- "confidence": float (0.0 to 1.0)'}

Only respond with the json object, nothing else.

**IMPORTANT:**
You must output a confidence score reflecting how likely the input is violative of the guardrail:
- 1.0 = Certain violative
- 0.0 = Certain not violative
- Use the full range [0.0 - 1.0] to reflect your level of certainty

Analyze the following text according to the instructions above.
`;
    return template.trim();
}
/**
 * Remove JSON code fencing (```json ... ```) from a response, if present.
 *
 * This function is defensive: it returns the input string unchanged unless
 * a valid JSON code fence is detected and parseable.
 *
 * @param text - LLM output, possibly wrapped in a JSON code fence.
 * @returns Extracted JSON string or the original string.
 */
function stripJsonCodeFence(text) {
    const lines = text.trim().split('\n');
    if (lines.length < 3) {
        return text;
    }
    const [first, ...body] = lines;
    const last = body.pop();
    if (!first?.startsWith('```json') || last !== '```') {
        return text;
    }
    const candidate = body.join('\n');
    try {
        JSON.parse(candidate);
    }
    catch {
        return text;
    }
    return candidate;
}
/**
 * Run an LLM analysis for a given prompt and user input.
 *
 * Invokes the OpenAI LLM, enforces prompt/response contract, parses the LLM's
 * output, and returns a validated result.
 *
 * @param text - Text to analyze.
 * @param systemPrompt - Prompt instructions for the LLM.
 * @param client - OpenAI client for LLM inference.
 * @param model - Identifier for which LLM model to use.
 * @param outputModel - Model for parsing and validating the LLM's response.
 * @returns Structured output containing the detection decision and confidence.
 */
async function runLLM(text, systemPrompt, client, model, outputModel) {
    const fullPrompt = buildFullPrompt(systemPrompt, outputModel);
    try {
        // Handle temperature based on model capabilities
        let temperature = 0.0;
        if (model.includes('gpt-5')) {
            // GPT-5 doesn't support temperature 0, use default (1)
            temperature = 1.0;
        }
        // Build API call parameters
        const params = {
            messages: [
                { role: 'system', content: fullPrompt },
                { role: 'user', content: `# Text\n\n${text}` },
            ],
            model: model,
            temperature: temperature,
            response_format: { type: 'json_object' },
        };
        // Only include safety_identifier for official OpenAI API (not Azure or local providers)
        if ((0, safety_identifier_1.supportsSafetyIdentifier)(client)) {
            // @ts-ignore - safety_identifier is not defined in OpenAI types yet
            params.safety_identifier = safety_identifier_1.SAFETY_IDENTIFIER;
        }
        // @ts-ignore - safety_identifier is not in the OpenAI types yet
        const response = await client.chat.completions.create(params);
        const result = response.choices[0]?.message?.content;
        if (!result) {
            return exports.LLMErrorOutput.parse({
                flagged: false,
                confidence: 0.0,
                info: {
                    error_message: 'LLM returned no content',
                },
            });
        }
        const cleanedResult = stripJsonCodeFence(result);
        return outputModel.parse(JSON.parse(cleanedResult));
    }
    catch (error) {
        console.error('LLM guardrail failed for prompt:', systemPrompt, error);
        // Check if this is a content filter error - Azure OpenAI
        if (error && typeof error === 'string' && error.includes('content_filter')) {
            console.warn('Content filter triggered by provider:', error);
            return exports.LLMErrorOutput.parse({
                flagged: true,
                confidence: 1.0,
                info: {
                    third_party_filter: true,
                    error_message: String(error),
                },
            });
        }
        // Fail-open on JSON parsing errors (malformed or non-JSON responses)
        if (error instanceof SyntaxError || error?.constructor?.name === 'SyntaxError') {
            console.warn('LLM returned non-JSON or malformed JSON.', error);
            return exports.LLMErrorOutput.parse({
                flagged: false,
                confidence: 0.0,
                info: {
                    error_message: 'LLM returned non-JSON or malformed JSON.',
                },
            });
        }
        // Fail-open on schema validation errors (e.g., wrong types like confidence as string)
        if (error instanceof zod_1.z.ZodError) {
            console.warn('LLM response validation failed.', error);
            return exports.LLMErrorOutput.parse({
                flagged: false,
                confidence: 0.0,
                info: {
                    error_message: 'LLM response validation failed.',
                    zod_issues: error.issues ?? [],
                },
            });
        }
        // Always return error information for other LLM failures
        return exports.LLMErrorOutput.parse({
            flagged: false,
            confidence: 0.0,
            info: {
                error_message: String(error),
            },
        });
    }
}
function isLLMErrorOutput(value) {
    if (!value || typeof value !== 'object') {
        return false;
    }
    if (!('info' in value)) {
        return false;
    }
    const info = value.info;
    if (!info || typeof info !== 'object') {
        return false;
    }
    return 'error_message' in info;
}
function createLLMCheckFn(name, description, systemPrompt, outputModel = exports.LLMOutput, configModel = exports.LLMConfig) {
    async function guardrailFunc(ctx, data, config) {
        let renderedSystemPrompt = systemPrompt;
        // Handle system_prompt_details if present (for user-defined LLM)
        if (config.system_prompt_details) {
            renderedSystemPrompt = systemPrompt.replace('{system_prompt_details}', config.system_prompt_details);
        }
        const analysis = await runLLM(data, renderedSystemPrompt, ctx.guardrailLlm, // Type assertion to handle OpenAI client compatibility
        config.model, outputModel);
        if (isLLMErrorOutput(analysis)) {
            return {
                tripwireTriggered: false,
                executionFailed: true,
                originalException: new Error(String(analysis.info?.error_message || 'LLM execution failed')),
                info: {
                    guardrail_name: name,
                    ...analysis,
                },
            };
        }
        const isTrigger = analysis.flagged && analysis.confidence >= config.confidence_threshold;
        return {
            tripwireTriggered: isTrigger,
            info: {
                guardrail_name: name,
                ...analysis,
                threshold: config.confidence_threshold,
            },
        };
    }
    registry_1.defaultSpecRegistry.register(name, guardrailFunc, description, 'text/plain', configModel, exports.LLMContext, { engine: 'LLM' });
    return guardrailFunc;
}
/**
 * Context requirements for LLM-based guardrails.
 */
exports.LLMContext = zod_1.z.object({
    guardrailLlm: zod_1.z.any(),
});
//# sourceMappingURL=llm-base.js.map