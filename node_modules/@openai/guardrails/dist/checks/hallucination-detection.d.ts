/**
 * Hallucination Detection guardrail module.
 *
 * This module provides a guardrail for detecting when an LLM generates content that
 * may be factually incorrect, unsupported, or "hallucinated." It uses the OpenAI
 * Responses API with file search to validate claims against actual documents.
 *
 * **IMPORTANT: A valid OpenAI vector store must be created before using this guardrail.**
 *
 * To create an OpenAI vector store, you can:
 *
 * 1. **Use the Guardrails Wizard**: Configure the guardrail through the [Guardrails Wizard](https://guardrails.openai.com/), which provides an option to create a vector store if you don't already have one.
 * 2. **Use the OpenAI Dashboard**: Create a vector store directly in the [OpenAI Dashboard](https://platform.openai.com/storage/vector_stores/).
 * 3. **Follow OpenAI Documentation**: Refer to the "Create a vector store and upload a file" section of the [File Search documentation](https://platform.openai.com/docs/guides/tools-file-search) for detailed instructions.
 * 4. **Use the provided utility script**: Use the `create_vector_store.py` script provided in the [repo](https://github.com/OpenAI-Early-Access/guardrails/blob/main/guardrails/src/guardrails/utils/create_vector_store.py) to create a vector store from local files or directories.
 *
 * **Pricing**: For pricing details on file search and vector storage, see the [Built-in tools section](https://openai.com/api/pricing/) of the OpenAI pricing page.
 */
import { z } from 'zod';
import { CheckFn, GuardrailLLMContext } from '../types';
/**
 * Configuration schema for hallucination detection.
 *
 * Extends the base LLM configuration with file search validation parameters.
 */
export declare const HallucinationDetectionConfig: z.ZodObject<{
    /** The LLM model to use for analysis (e.g., "gpt-4o-mini") */
    model: z.ZodString;
    /** Minimum confidence score (0.0 to 1.0) required to trigger the guardrail. Defaults to 0.7. */
    confidence_threshold: z.ZodDefault<z.ZodNumber>;
    /** Vector store ID to use for document validation (must start with 'vs_') */
    knowledge_source: z.ZodString;
}, "strip", z.ZodTypeAny, {
    model: string;
    confidence_threshold: number;
    knowledge_source: string;
}, {
    model: string;
    knowledge_source: string;
    confidence_threshold?: number | undefined;
}>;
export type HallucinationDetectionConfig = z.infer<typeof HallucinationDetectionConfig>;
/**
 * Context requirements for the hallucination detection guardrail.
 */
export type HallucinationDetectionContext = GuardrailLLMContext;
/**
 * Output schema for hallucination detection analysis.
 */
export declare const HallucinationDetectionOutput: z.ZodObject<{
    /** Whether the content was flagged as potentially hallucinated */
    flagged: z.ZodBoolean;
    /** Confidence score (0.0 to 1.0) that the input is hallucinated */
    confidence: z.ZodNumber;
    /** Detailed explanation of the analysis */
    reasoning: z.ZodString;
    /** Type of hallucination detected */
    hallucination_type: z.ZodNullable<z.ZodString>;
    /** Specific statements flagged as potentially hallucinated */
    hallucinated_statements: z.ZodNullable<z.ZodArray<z.ZodString, "many">>;
    /** Specific statements that are supported by the documents */
    verified_statements: z.ZodNullable<z.ZodArray<z.ZodString, "many">>;
}, "strip", z.ZodTypeAny, {
    flagged: boolean;
    confidence: number;
    reasoning: string;
    hallucination_type: string | null;
    hallucinated_statements: string[] | null;
    verified_statements: string[] | null;
}, {
    flagged: boolean;
    confidence: number;
    reasoning: string;
    hallucination_type: string | null;
    hallucinated_statements: string[] | null;
    verified_statements: string[] | null;
}>;
export type HallucinationDetectionOutput = z.infer<typeof HallucinationDetectionOutput>;
/**
 * Detect potential hallucinations in text by validating against documents.
 *
 * This function uses the OpenAI Responses API with file search and structured output
 * to validate factual claims in the candidate text against the provided knowledge source.
 * It flags content that contains any unsupported or contradicted factual claims.
 *
 * @param ctx Guardrail context containing the LLM client.
 * @param candidate Text to analyze for potential hallucinations.
 * @param config Configuration for hallucination detection.
 * @returns GuardrailResult containing hallucination analysis with flagged status
 *         and confidence score.
 */
export declare const hallucination_detection: CheckFn<HallucinationDetectionContext, string, HallucinationDetectionConfig>;
//# sourceMappingURL=hallucination-detection.d.ts.map