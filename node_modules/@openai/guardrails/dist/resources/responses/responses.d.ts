/**
 * Responses API with guardrails.
 */
import { OpenAI } from 'openai';
import { GuardrailsBaseClient, GuardrailsResponse } from '../../base-client';
import { Message } from '../../types';
/**
 * Responses API with guardrails.
 */
export declare class Responses {
    private client;
    constructor(client: GuardrailsBaseClient);
    /**
     * Create response with guardrails.
     *
     * Runs preflight first, then executes input guardrails concurrently with the LLM call.
     */
    create(params: {
        input: string | Message[];
        model: string;
        stream: true;
        tools?: unknown[];
        suppressTripwire?: boolean;
    } & Omit<OpenAI.Responses.ResponseCreateParams, 'input' | 'model' | 'stream' | 'tools'>): Promise<AsyncIterableIterator<GuardrailsResponse>>;
    create(params: {
        input: string | Message[];
        model: string;
        stream?: false;
        tools?: unknown[];
        suppressTripwire?: boolean;
    } & Omit<OpenAI.Responses.ResponseCreateParams, 'input' | 'model' | 'stream' | 'tools'>): Promise<GuardrailsResponse<OpenAI.Responses.Response>>;
}
//# sourceMappingURL=responses.d.ts.map