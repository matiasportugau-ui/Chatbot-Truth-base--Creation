/**
 * Guardrail evaluation runner and CLI.
 *
 * This script provides a command-line interface and class for running guardrail evaluations on datasets.
 */
import { Sample } from './core/types';
/**
 * Class for running guardrail evaluations.
 */
export declare class GuardrailEval {
    private configPath;
    private datasetPath;
    private stages;
    private batchSize;
    private outputDir;
    private apiKey;
    private baseUrl;
    private azureEndpoint;
    private azureApiVersion;
    private mode;
    private models;
    private latencyIterations;
    private multiTurn;
    private maxParallelModels;
    private benchmarkChunkSize;
    /**
     * Initialize the evaluator.
     *
     * @param configPath - Path to pipeline configuration file
     * @param datasetPath - Path to evaluation dataset (JSONL)
     * @param stages - Specific stages to evaluate (pre_flight, input, output)
     * @param batchSize - Number of samples to process in parallel
     * @param outputDir - Directory to save evaluation results
     * @param apiKey - API key for OpenAI, Azure OpenAI, or OpenAI-compatible API
     * @param baseUrl - Base URL for OpenAI-compatible API (e.g., http://localhost:11434/v1)
     * @param azureEndpoint - Azure OpenAI endpoint (e.g., https://your-resource.openai.azure.com)
     * @param azureApiVersion - Azure OpenAI API version (e.g., 2025-01-01-preview)
     * @param mode - Evaluation mode ("evaluate" or "benchmark")
     * @param models - Models to test in benchmark mode
     * @param latencyIterations - Number of iterations for latency testing
     * @param multiTurn - Whether to evaluate guardrails on multi-turn conversations
     * @param maxParallelModels - Maximum number of models to benchmark concurrently
     * @param benchmarkChunkSize - Optional sample chunk size for per-model benchmarking
     */
    constructor(configPath: string, datasetPath: string, stages?: string[] | null, batchSize?: number, outputDir?: string, apiKey?: string | null, baseUrl?: string | null, azureEndpoint?: string | null, azureApiVersion?: string, mode?: 'evaluate' | 'benchmark', models?: string[] | null, latencyIterations?: number, multiTurn?: boolean, maxParallelModels?: number | null, benchmarkChunkSize?: number | null);
    private _validateFilePaths;
    /**
     * Resolve the number of benchmark tasks that can run concurrently.
     *
     * @param modelCount - Total number of models scheduled for benchmarking
     * @param requestedLimit - Optional user-provided parallelism limit
     * @returns Number of concurrent benchmark tasks to run
     */
    static _determineParallelModelLimit(modelCount: number, requestedLimit?: number | null): number;
    /**
     * Yield contiguous sample chunks respecting the configured chunk size.
     *
     * @param samples - Samples to evaluate
     * @param chunkSize - Optional maximum chunk size to enforce
     * @returns Generator yielding slices of the provided samples
     */
    static _chunkSamples(samples: Sample[], chunkSize?: number | null): Generator<Sample[], void, unknown>;
    /**
     * Run the evaluation pipeline for all specified stages.
     */
    run(): Promise<void>;
    private _runEvaluation;
    private _runBenchmark;
    private _hasModelConfiguration;
    private _runLatencyTests;
    private _createContext;
    private _isValidStage;
    /**
     * Create a modified copy of a stage bundle with model-specific configuration.
     *
     * @param stageBundle - Original stage bundle
     * @param model - Model name to inject into guardrail configs
     * @returns Modified stage bundle with updated model configuration
     */
    private _createModelSpecificStageBundle;
    private _getValidStages;
    private _evaluateSingleStage;
    private _getBenchmarkTarget;
    private _benchmarkAllModels;
    private _benchmarkSingleModel;
}
/**
 * CLI entry point for running evaluations.
 *
 * @param args - Command line arguments
 */
export declare function runEvaluationCLI(args: {
    configPath: string;
    datasetPath: string;
    stages?: string[] | null;
    batchSize?: number;
    outputDir?: string;
    apiKey?: string | null;
    baseUrl?: string | null;
    azureEndpoint?: string | null;
    azureApiVersion?: string;
    mode?: 'evaluate' | 'benchmark';
    models?: string[] | null;
    latencyIterations?: number;
    multiTurn?: boolean;
    maxParallelModels?: number | null;
    benchmarkChunkSize?: number | null;
}): Promise<void>;
//# sourceMappingURL=guardrail-evals.d.ts.map