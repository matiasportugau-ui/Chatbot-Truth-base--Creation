/**
 * Visualization module for guardrail benchmarking.
 *
 * This module generates charts and graphs for benchmark results.
 * Note: Full visualization requires additional plotting libraries.
 * This is a stub implementation that matches the Python interface.
 */
/**
 * Generates visualizations for guardrail benchmark results.
 */
export declare class BenchmarkVisualizer {
    private readonly outputDir;
    /**
     * Initialize the visualizer.
     *
     * @param outputDir - Directory to save generated charts
     */
    constructor(outputDir: string);
    /**
     * Create all visualizations for a benchmark run.
     *
     * @param resultsByModel - Dictionary mapping model names to their results
     * @param metricsByModel - Dictionary mapping model names to their metrics
     * @param latencyResults - Dictionary mapping model names to their latency data
     * @param guardrailName - Name of the guardrail being evaluated
     * @param _expectedTriggers - Expected trigger values for each sample (reserved for future use)
     * @returns List of paths to saved visualization files
     */
    createAllVisualizations(resultsByModel: Record<string, unknown[]>, metricsByModel: Record<string, Record<string, number>>, latencyResults: Record<string, Record<string, unknown>>, guardrailName: string, _expectedTriggers: Record<string, boolean>): Promise<string[]>;
}
//# sourceMappingURL=visualizer.d.ts.map