/**
 * Benchmark results reporter for guardrail evaluation.
 *
 * This module handles saving benchmark results in a specialized format with analysis
 * folders containing visualizations and detailed metrics.
 */
import { SampleResult } from './types';
/**
 * Reports benchmark results with specialized output format.
 */
export declare class BenchmarkReporter {
    private readonly outputDir;
    /**
     * Initialize the benchmark reporter.
     *
     * @param outputDir - Base directory for benchmark results
     */
    constructor(outputDir: string);
    /**
     * Save benchmark results in organized folder structure.
     *
     * @param resultsByModel - Dictionary mapping model names to their results
     * @param metricsByModel - Dictionary mapping model names to their metrics
     * @param latencyResults - Dictionary mapping model names to their latency data
     * @param guardrailName - Name of the guardrail being benchmarked
     * @param datasetSize - Number of samples in the dataset
     * @param latencyIterations - Number of iterations used for latency testing
     * @returns Path to the benchmark results directory
     */
    saveBenchmarkResults(resultsByModel: Record<string, SampleResult[]>, metricsByModel: Record<string, Record<string, number>>, latencyResults: Record<string, Record<string, unknown>>, guardrailName: string, datasetSize: number, latencyIterations: number): Promise<string>;
    private createPerformanceTable;
    private createLatencyTable;
    private formatTable;
    private saveSummaryTables;
    private saveResultsJsonl;
    private saveMetricsJson;
    private saveLatencyJson;
    private saveBenchmarkSummary;
}
//# sourceMappingURL=benchmark-reporter.d.ts.map